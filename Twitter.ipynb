{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recolección datos de Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rate limites: https://developer.twitter.com/en/docs/basics/rate-limits.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión con la API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser  \n",
    "import sys\n",
    "from tweepy import API # conda install -c conda-forge tweepy \n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "def get_twitter_auth():\n",
    "    \"\"\"Configura la autenticación con Twitter\n",
    "\n",
    "    Retorna: un objeto tweepy.OAuthHandler\n",
    "    \"\"\"\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(\"d://notebooks//webmining//twitter.ini\")\n",
    "    try:\n",
    "        consumer_key = config.get(\"TwitterKeys\", \"ConsumerKey\")\n",
    "        consumer_secret = config.get(\"TwitterKeys\", \"ConsumerSecret\")\n",
    "        access_token = config.get(\"TwitterKeys\", \"AccessToken\")\n",
    "        access_secret = config.get(\"TwitterKeys\", \"AccessTokenSecret\")\n",
    "    except:\n",
    "        print(\"exception on %s!\" % option)\n",
    "        #sys.exit(1)  \n",
    "        \n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    return auth\n",
    "\n",
    "def get_twitter_client():\n",
    "    \"\"\"Configura un cliente de la API de Twitter.\n",
    "\n",
    "    Retorna: objeto tweepy.API\n",
    "    \"\"\"\n",
    "    auth = get_twitter_auth()\n",
    "    client = API(auth, wait_on_rate_limit=True)\n",
    "    return client\n",
    "\n",
    "auth = get_twitter_auth()\n",
    "client = API(auth)\n",
    "print(\"Se puede otorgar acceso a esta aplicación con el siguiente link: \")\n",
    "print(auth.get_authorization_url(),\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método **get_twitter_auth** es el encargado de la autenticación de la aplicación.\n",
    "\n",
    "**Os** posee un diccionario environ con las variables de entorno, que se acceden por clave. Si alguna de las variables falta, surge una **KeyError**\n",
    "\n",
    "El método **get_twitter_client** se usa para crear una instancia de la API de Tweepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leer tweets de los diferentes timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home timeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que podemos probar es obtener los primeros 10 tweets de nuestro propio **home timeline**.\n",
    "\n",
    "El home timeline es lo que vemos cuando ingresamos a twitter, y contiene una secuencia de tweets de los usuarios que seguimos (los más recientes primero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Cursor\n",
    "\n",
    "client = get_twitter_client()\n",
    "for status in Cursor(client.home_timeline).items(10):\n",
    "    # Procesamos un Status a la vez\n",
    "    print(status.text,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tweepy.Status** es el modelo usado por Tweepy para encapsular los tweets. En este caso accedemos al texto, pero tiene otros datos como veremos más adelante.\n",
    "\n",
    "**tweepy.Cursor** es un objeto iterable que facilita la iteración y paginación de resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tweepy import Cursor\n",
    "\n",
    "with open('d://notebooks//webmining//home_timeline.jsonl', 'w') as f:\n",
    "    for page in Cursor(client.home_timeline, count=200).pages(4):\n",
    "        for status in page:\n",
    "            f.write(json.dumps(status._json)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo iteramos 4 paginas de 200 tweets. La razón es por una limitación de Twitter: solo podemos recuperar los 800 tweets más recientes de nuestro home timeline.\n",
    "\n",
    "El formato jsonl es JSON LINES, representa un JSON valido por cada línea, útil para procesar grandes volúmenes de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"pottermore\"\n",
    "client = get_twitter_client()\n",
    "fname = \"d://notebooks//webmining//user_timeline_{}.jsonl\".format(user)\n",
    "page_no = 1\n",
    "with open(fname, 'w') as f:\n",
    "    for page in Cursor(client.user_timeline, screen_name=user, count=200).pages(16):\n",
    "        print(\"Descargando página {} con {} tweets\".format(page_no, len(page)))    \n",
    "        page_no+=1\n",
    "        for status in page:\n",
    "            f.write(json.dumps(status._json)+\"\\n\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tweet = client.user_timeline(client.user_timeline, screen_name=user, count = 2)[0]\n",
    "print(json.dumps(tweet._json, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables enteras tienen también una variante en formato string para los lenguajes que no soportan enteros de 64 bits.\n",
    "\n",
    "**entities** es un diccionario que contiene diferentes entidades reconocidas en el tweet, como por ejemplo hashtags, fotos, urls y menciones a usuario\n",
    "\n",
    "Se puede observar que toda la información del usuario está contenida dentro del mismo tweet (redundancia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función auxiliar para imprimir con formato\n",
    "from IPython.display import Markdown, display\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "client = get_twitter_client()\n",
    "fname = \"d://notebooks//webmining//public_timeline_query.jsonl\"\n",
    "with open(fname, 'w') as f:\n",
    "    for tweet in Cursor(client.search,q=\"#Rusia2018\",count=100,lang=\"en\",since=\"2018-01-01\").items(10): \n",
    "        f.write(json.dumps(tweet._json)+\"\\n\")\n",
    "        printmd(\"Tweet de <b>{}</b>: {}\".format(tweet.user.screen_name, tweet.text))\n",
    "        print(\"-------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La API de streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import time\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente clase extiende de StreamListener y redefine dos métodos: **on_data()** y **on_error()** que son handlers que se ejecutan cuando llegan nuevos datos y cuando surge un error, respectivamente. Ambos métodos retornan un booleano indicando si se debe continuar escuchando el stream (true) o si ocurrió un error fatal y se debe interrumpir la escucha (false)\n",
    "\n",
    "Puede chequearse la lista de errores en la documentación de la API de Twitter: \n",
    "\n",
    "https://developer.twitter.com/en/docs/basics/response-codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomListener(StreamListener):\n",
    "    \"\"\"Custom StreamListener for streaming Twitter data.\"\"\"\n",
    "    def __init__(self, fname, start_time, limit):\n",
    "        safe_fname = format_filename(fname)\n",
    "        self.outfile = \"d://notebooks//webmining//stream_{}_{}.jsonl\".format(limit, safe_fname)        \n",
    "        self.limit = limit\n",
    "        self.start_time = start_time\n",
    "        self.num_tweets = 0\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        if (self.num_tweets < self.limit):\n",
    "            self.num_tweets += 1\n",
    "            try:\n",
    "                with open(self.outfile, 'a') as f:\n",
    "                    f.write(data)\n",
    "                    return True\n",
    "            except BaseException as e:\n",
    "                sys.stderr.write(\"Error on_data: {}\\n\".format(e))\n",
    "                time.sleep(5)\n",
    "                return True\n",
    "        else:\n",
    "            print (\"{} tweets descargados en {} segs\".format(self.num_tweets, time.time()-start_time))\n",
    "            return False\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            sys.stderr.write(\"Rate limit exceeded\\n\")\n",
    "            return False\n",
    "        else:\n",
    "            sys.stderr.write(\"Error {}\\n\".format(status))\n",
    "            return True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos dos funciones auxiliares para eliminar caracteres inválidos de los nombres de archivos y reemplazarlos por _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_filename(fname):\n",
    "    \"\"\"Convert fname into a safe string for a file name.\n",
    "\n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    return ''.join(convert_valid(one_char) for one_char in fname)\n",
    "\n",
    "\n",
    "def convert_valid(one_char):\n",
    "    \"\"\"Convert a character into '_' if \"invalid\".\n",
    "\n",
    "    Return: string\n",
    "    \"\"\"\n",
    "    valid_chars = \"-_.%s%s\" % (string.ascii_letters, string.digits)\n",
    "    if one_char in valid_chars:\n",
    "        return one_char\n",
    "    else:\n",
    "        return '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = [\"\\#Rusia2018\", \"WorldCup\"] # list of arguments\n",
    "query_fname = ' '.join(query) # string\n",
    "auth = get_twitter_auth()\n",
    "start_time = time.time()\n",
    "twitter_stream = Stream(auth, CustomListener(query_fname, start_time, limit=100))\n",
    "twitter_stream.filter(track=query, async=True, languages=[\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_stream.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener, por ejemplo, los 20 hashtags más frecuentes. \n",
    "\n",
    "La clase **Counter**, es una subclase de **Dict** que se encarga de contar la cantidad de ocurrencias de cada clave.\n",
    "\n",
    "Usamos una función auxiliar **get_hashtags** que recibe un tweet completo y obtiene los hashtags. En lugar de acceder directamente al atributo del tweet con **tweet['entities']** usamos el método get al que podemos indicarle el valor por defecto en caso de que el atributo no se encuentre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "def get_hashtags(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('hashtags', []) \n",
    "    # cada hashtags tiene la  siguiente estructura {'text': 'Herbology', 'indices': [127, 137]}\n",
    "    # solo nos interesa el text\n",
    "    return [tag['text'].lower() for tag in hashtags]\n",
    "\n",
    "fname = \"user_timeline_pottermore.jsonl\"\n",
    "fname = \"stream_10000___Rusia2018_WorldCup_saved.jsonl\"\n",
    "with open(fname, 'r') as f:\n",
    "    hashtags = Counter() \n",
    "    for line in f:\n",
    "        tweet = json.loads(line) #Leemos la linea del jsonl (un json en si misma)\n",
    "        hashtags_in_tweet = get_hashtags(tweet) # obtenemos los hashtags del tweet\n",
    "        hashtags.update(hashtags_in_tweet) # actualizamos el Counter\n",
    "    for tag, count in hashtags.most_common(20):\n",
    "        print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos obtener una estadística más descriptiva con el siguiente script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(fname, 'r') as f:\n",
    "    hashtag_count = defaultdict(int)\n",
    "    \n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        hashtags_in_tweet = get_hashtags(tweet)\n",
    "        n_of_hashtags = len(hashtags_in_tweet)\n",
    "        hashtag_count[n_of_hashtags] += 1\n",
    "        \n",
    "    tweets_with_hashtags = sum([count for n_of_tags, count in hashtag_count.items() if n_of_tags > 0])\n",
    "    tweets_no_hashtags = hashtag_count[0]\n",
    "    tweets_total = tweets_no_hashtags + tweets_with_hashtags\n",
    "    print(\"Cantidad de tweets\")\n",
    "    tweets_with_hashtags_percent = \"%.2f\" % (tweets_with_hashtags / tweets_total * 100)\n",
    "    tweets_no_hashtags_percent = \"%.2f\" % (tweets_no_hashtags / tweets_total * 100)\n",
    "    print(\"{} tweets sin hashtags ({}%)\".format(tweets_no_hashtags, tweets_no_hashtags_percent))\n",
    "    print(\"{} tweets con al menos un  hashtag ({}%)\".format(tweets_with_hashtags, tweets_with_hashtags_percent))\n",
    "    \n",
    "    for tag_count, tweet_count in hashtag_count.items():\n",
    "        if tag_count > 0:\n",
    "            percent_total = \"%.2f\" % (tweet_count / tweets_total * 100)\n",
    "            percent_elite = \"%.2f\" % (tweet_count / tweets_with_hashtags * 100)\n",
    "            print(\"{} tweets con {} hashtags ({}% del total de tweets, {}% del total con hashtags)\".format(tweet_count, tag_count, percent_total, percent_elite))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma similar podemos observar las menciones a diferentes usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mentions(tweet):\n",
    "    entities = tweet.get('entities', {})\n",
    "    hashtags = entities.get('user_mentions', [])\n",
    "    return [tag['screen_name'] for tag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname, 'r') as f:\n",
    "    users = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        mentions_in_tweet = get_mentions(tweet)\n",
    "        users.update(mentions_in_tweet)\n",
    "    for user, count in users.most_common(20):\n",
    "        print(\"{}: {}\".format(user, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar NLTK para analizar el texto de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import string\n",
    "import json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cada tweet recibido (solo el texto), usamos el TweetTokenizer para separarlo en palabras y filtramos stopwords recibidas por parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, tokenizer=TweetTokenizer(), stopwords=[]):\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return [tok for tok in tokens if tok not in stopwords and not tok.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el tokenizador de Tweets (recordar clase anterior) y usamos las stopwords comunes de ingles de nltk, más los signos de puntuación, más un conjunto de stopwords del dominio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...', '…']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por cada tweet levantado del archivo jsonl, lo procesamos y actualizamos la cantidad de veces que aparece cada palabra. Luego imprimimos las 30 palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"user_timeline_pottermore.jsonl\"\n",
    "tf = Counter()\n",
    "with open(fname, 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = process(text=tweet.get('text', ''),\n",
    "                         tokenizer=tweet_tokenizer,\n",
    "                         stopwords=stopword_list)\n",
    "        tf.update(tokens)\n",
    "    for tag, count in tf.most_common(30):\n",
    "        print(\"{}: {}\".format(tag, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el resultado incluye una mezcla de palabras, menciones a usuarios, ulrs y simbolos no incluidos como puntuación. Estos simbolos se pueden eliminar simplemente extendiendo la lista de stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos extender los scripts anteriores generando un gráfico de frecuencias con matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "punct = list(string.punctuation)\n",
    "stopword_list = stopwords.words('english') + punct + ['rt', 'via', '...', '…']\n",
    "\n",
    "fname = \"user_timeline_pottermore.jsonl\"\n",
    "tf = Counter()\n",
    "with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            tweet = json.loads(line)\n",
    "            tokens = process(text=tweet.get('text', ''),\n",
    "                             tokenizer=tweet_tokenizer,\n",
    "                             stopwords=stopword_list)\n",
    "            tf.update(tokens)\n",
    "        y = [count for tag, count in tf.most_common(1000)]\n",
    "        x = range(1, len(y)+1)\n",
    "        plt.bar(x, y)\n",
    "        plt.title(\"Term Frequencies\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que hay pocos términos que aparecen muchas veces (a la izquierda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de series temporales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En términos generales, una serie temporal es una secuencia de datos que consisten en sucesivas observaciones a lo largo de un período de tiempo. Como Twitter provee para cada tweet el atributo **created_at** podemos ordenar los tweets en el tiempo para estudiar, por ejemplo, cómo los usuario reaccionan a eventos en tiempo real (por ejemplo eventos deportivos, conciertos, elecciones políticas, catátstrofes, programas televisivos, etc.).\n",
    "\n",
    "Otra posible aplicación es estudiar qué opinan los usuarios acerca de un producto o una marca.\n",
    "\n",
    "Como nos interesa estudiar la masa de usuarios y no un usuario individual, conviene utilizar la api de Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"stream___Rusia2018_WorldCup_saved.jsonl\"\n",
    "fname = \"stream_10000___Rusia2018_WorldCup_saved.jsonl\"\n",
    "with open(fname, 'r') as f:\n",
    "    all_dates = []\n",
    "    #i=1\n",
    "    for line in f:\n",
    "        #print(\"reading line \",i)\n",
    "        #i+=1\n",
    "        tweet = json.loads(line)\n",
    "        all_dates.append(tweet.get('created_at'))\n",
    "    \n",
    "    ones = np.ones(len(all_dates)) # un arreglo de todos unos\n",
    "    idx = pd.DatetimeIndex(all_dates) #serie de datos, indexada por las fechas observadas en los tweets    \n",
    "    my_series = pd.Series(ones, index=idx) # la serie en si (por el momento, todos unos)\n",
    "\n",
    "    # Resampleo/agrupamiento en ventanas de 1 minuto\n",
    "    per_minute = my_series.resample('1Min').sum().replace(np.nan, 0)\n",
    "    print(my_series.head())\n",
    "    print(per_minute.head())\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(True)\n",
    "    ax.set_title(\"Tweet Frequencies\")\n",
    "\n",
    "    hours = mdates.MinuteLocator(interval=20)\n",
    "    date_formatter = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "    #datemin = datetime(2018, 5, 22, 20, 32)\n",
    "    #datemax = datetime(2018, 5, 22, 20, 40)\n",
    "\n",
    "    ax.xaxis.set_major_locator(hours)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    #ax.set_xlim(datemin, datemax)\n",
    "    max_freq = per_minute.max()\n",
    "    ax.set_ylim(0, max_freq)\n",
    "    ax.plot(per_minute.index, per_minute)\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = [20,9]\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users, Followers y Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las grandes diferencias entre Twitter y otros medios sociales (por ejemplo Facebook o LinkedIn) es la forma en la que los usuarios se conectan entre sí. Las relaciones entre los usuarios de Twitter son unidireccionales. Esto quiere decir que un usuario puede conectarse con otros sin necesidad de que éstos últimos acepten la conexión.\n",
    "En Twitter los usuarios con los que un usuario A se conecta se denominan **friends** mientras que los que se conectan con el usuario A se denominan **followers** desde el punto de vista de A. Cuando la conexión es bidireccional hablamos de **mutual friends**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de un usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una de las posibilidades básicas de la API de Twitter es ver el perfil de un usuario dado. Esta actividad está limitada actualmente a 900 requests cada 15 minutos.\n",
    "\n",
    "El método **get_user** de la API, recibe un nombre de usuario y retorna un objeto del tipo **tweepy.models.User**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "client = get_twitter_client()\n",
    "profile = client.get_user(screen_name=\"PotterMore\")\n",
    "print(json.dumps(profile._json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La \"red\" de un usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden ver las listas de followers y de amigos con **API.followers()** y **API.friends()** pero hay una limitación muy grande de 15 request cada 15 minutos donde cada request sólo retorna hasta 20 perfiles de usuario. Esto nos da una limitación de 300 perfiles cada 15 minutos, que hace imposible usar esta técnica para descargar grandes volúmenes de datos.\n",
    "\n",
    "Un *truco* es utilizar **API.followers_ids** y **API.friends_ids** que retorna grupos de 5000 IDs por request (un total de 75000 cada 15 minutos). Con este método solo tendremos los identificadores de usuario, pero luego podemos usar estos ID para obtener la información de los usuarios con el método **lookup** que toma listas de hasta 100 IDs como entrada y retorna los perfiles completos como salida. El método **lookup** tiene una limitación de 300 pedidos cada 15 minutos, lo que nos da un total de 30000 perfiles cada 15 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "from tweepy import Cursor\n",
    "\n",
    "MAX_FRIENDS = 1500\n",
    "\n",
    "screen_name = \"mg_armentano\"\n",
    "client = get_twitter_client()\n",
    "dirname = \"users/{}\".format(screen_name)\n",
    "\n",
    "max_pages = math.ceil(MAX_FRIENDS / 5000)\n",
    "try:\n",
    "    os.makedirs(dirname, mode=0o755, exist_ok=True)\n",
    "except OSError:\n",
    "    print(\"El directorio {} ya existe\".format(dirname))\n",
    "except Exception as e:\n",
    "    print(\"Error al crear el directorio {}\".format(dirname))\n",
    "    print(e)\n",
    "    sys.exit(1)\n",
    "    \n",
    "def paginate(items, n):\n",
    "    \"\"\"Genera bloques de tamaño n a partir de los items\"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los seguidores del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"users/{}/followers.jsonl\".format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    for followers in Cursor(client.followers_ids, screen_name=screen_name).pages(max_pages):\n",
    "        for chunk in paginate(followers, 100):\n",
    "            print(\"Descargando 100 followers de un total de \",len(followers))\n",
    "            users = client.lookup_users(user_ids=chunk)\n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+\"\\n\")            \n",
    "            if len(followers) == 5000:\n",
    "                print(\"Hay más resultados, pero esperamos 60 segundos para evitar el rate limit\")\n",
    "                time.sleep(60)            \n",
    "        print(\"Página terminada!\")\n",
    "    print(\"Todo descargado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muy similar para los amigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"users/{}/friends.jsonl\".format(screen_name)\n",
    "with open(fname, 'w') as f:\n",
    "    for friends in Cursor(client.friends_ids, screen_name=screen_name).pages(max_pages):\n",
    "        for chunk in paginate(friends, 100):\n",
    "            print(\"Descargando 100 friends de un total de \",len(friends))\n",
    "            users = client.lookup_users(user_ids=chunk)\n",
    "            for user in users:\n",
    "                f.write(json.dumps(user._json)+\"\\n\")\n",
    "            print(\"Bloque terminado!\")\n",
    "            if len(friends) == 5000:\n",
    "                print(\"Hay más resultados, pero esperamos 60 segundos para evitar el rate limit\")\n",
    "                time.sleep(60)            \n",
    "        print(\"Página terminada!\")\n",
    "    print(\"Todo descargado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos el perfil de cada usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name=\"mg_armentano\"\n",
    "followers_file = \"users/{}/followers.jsonl\".format(user_name)\n",
    "with open(followers_file) as f1:\n",
    "    for line in f1:\n",
    "        profile = json.loads(line)\n",
    "        screen_name = profile['screen_name']\n",
    "        print(\"Descargando {}\".format(screen_name))\n",
    "        \n",
    "        fname = \"users/{}.json\".format(screen_name)\n",
    "        with open(fname, 'w') as f:\n",
    "            profile = client.get_user(screen_name=screen_name)\n",
    "            f.write(json.dumps(profile._json, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de la red descargada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que tenemos descargada la inforamción de la red, podemos realizar algunas estadísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "\n",
    "screen_name = \"mg_armentano\"\n",
    "followers_file = 'users/{}/followers.jsonl'.format(screen_name)\n",
    "friends_file = 'users/{}/friends.jsonl'.format(screen_name)\n",
    "with open(followers_file) as f1, open(friends_file) as f2:\n",
    "    followers = []\n",
    "    friends = []\n",
    "    for line in f1:\n",
    "        profile = json.loads(line)\n",
    "        followers.append(profile['screen_name'])\n",
    "    for line in f2:\n",
    "        profile = json.loads(line)\n",
    "        friends.append(profile['screen_name'])\n",
    "    mutual_friends = [user for user in friends if user in followers]\n",
    "    followers_not_following = [user for user in followers if user not in friends]\n",
    "    friends_not_following = [user for user in friends if user not in followers]\n",
    "    print(\"{} tiene {} seguidores\".format(screen_name, len(followers)))\n",
    "    print(\"{} tiene {} amigos\".format(screen_name, len(friends)))\n",
    "    print(\"{} tiene {} amigos mutuos\".format(screen_name, len(mutual_friends)))\n",
    "    print(\"{} amigos no siguen a {}\".format(len(friends_not_following), screen_name))\n",
    "    print(\"{} seguidores no son seguidos por {}\".format(len(followers_not_following), screen_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunidades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la vida real, una comunidad es un grupo de personas con algo en común. Esta definición tan general incluye, por ejemplo, comunidades de gente que trabaja en determinada area geográfica, gente con intereses políticos similares, gente de la misma religión, gente que comparte un interés común, etc.\n",
    "\n",
    "En las redes sociales online también surgen comunidades de este tipo cuando gente con intereses comunes comienzan a interactuar.\n",
    "\n",
    "Las comunidades pueden ser **explícitas**, cuando cada miembro sabe exactamente si pertenece o no a una comunidad y generalmente conoce quiénes son los demás miembros de la comunidad, o **implícitas** cuando las comunidades surgen simplemente por las interacciones entre los miembros.\n",
    "\n",
    "Una de las técnicas utilizadas para descubrir comunidades implícitas es el **clustering** de usuarios. Podemos por ejemplo, agrupar los seguidores de un usuario utilizando la descripción dada en sus perfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "connections_file=\"users/mg_armentano/followers.jsonl\"\n",
    "with open(connections_file) as f:\n",
    "    users = []\n",
    "    # Carga de datos\n",
    "    for line in f:\n",
    "        profile = json.loads(line)\n",
    "        users.append(profile['description'])\n",
    "\n",
    "    # Creación de los vectores de términos\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                 min_df=2, \n",
    "                                 max_features=200, \n",
    "                                 stop_words='english', \n",
    "                                 ngram_range=(1,3), \n",
    "                                 use_idf=True)\n",
    "    X = vectorizer.fit_transform(users)\n",
    "    print(\"Dimensiones de los datos: {}\".format(X.shape))\n",
    "\n",
    "    # Aprendizaje del modelo usando K-Means\n",
    "    km = KMeans(n_clusters=5)\n",
    "    km.fit(X)\n",
    "    clusters = defaultdict(list)\n",
    "    for i, label in enumerate(km.labels_):\n",
    "        clusters[label].append(users[i])\n",
    "\n",
    "    # imprimimos 10 descripciones de usuarios de cada cluster\n",
    "    for label, descriptions in clusters.items():\n",
    "        print('---------- Cluster {}'.format(label+1))\n",
    "        for desc in descriptions[:10]:\n",
    "            print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Twitter, los usuarios pueden interactuar respondiendo a un tweet particular. Cuando dos o más usuarios utilizan este mecanismo, se genera una \"conversación\". Las conversaciones generan un grafo dirigido acíclico (DAG) por lo que podemos aplicar algoritmos de grafos sobre ellas. \n",
    "\n",
    "Por ejemplo, el **grado** de un nodo es la cantidad de nodos en el grafo que son hijos del mismo. Desde el punto de vista de una conversación, estamos hablando de la cantidad de respuestas recibidas. Otro concepto relacionado es el de **camino** que representa la secuencia de nodos intermedios que conectan dos nodos dados. Podemos por ejemplo encontrar el camino más largo a partir de un nodo (en este contexto, la conversación más extensa)\n",
    "\n",
    "Dado un jsonl conteniendo tweets, podemos construir el grafo utilizando la biblioteca de Python NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "\n",
    "#fname = \"stream___Rusia2018_WorldCup.jsonl\"\n",
    "#fname = \"user_timeline_pottermore.jsonl\"\n",
    "fname = \"stream_10000___Rusia2018_WorldCup_saved.jsonl\"\n",
    "with open(fname) as f:\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if 'id' in tweet:\n",
    "            graph.add_node(tweet['id'], #este es el único cambo obligatorio, los demás agregan info al nodo\n",
    "                           tweet=tweet['text'],\n",
    "                           author=tweet['user']['screen_name'],\n",
    "                           created_at=tweet['created_at'])\n",
    "\n",
    "            #Chequeamos si el tweet es una respuesta, para crear un arco\n",
    "            if tweet['in_reply_to_status_id']:            \n",
    "                reply_to = tweet['in_reply_to_status_id']\n",
    "                #Ignoramos si es una auto-respuesta (para no tener en cuenta \"monólogos\")\n",
    "                if reply_to in graph and tweet['user']['screen_name'] != graph.node[reply_to]['author']:\n",
    "                    graph.add_edge(tweet['in_reply_to_status_id'], tweet['id'])\n",
    "    \n",
    "    # Imprimimos algunas estadísticas\n",
    "    print(nx.info(graph))\n",
    "    \n",
    "    # Buscamos el tweet con más respuestas\n",
    "    sorted_replied = sorted(graph.degree(), key=itemgetter(1), reverse=True)\n",
    "    most_replied_id, replies = sorted_replied[0]\n",
    "    print(\"\\nEl tweet con más respuestas ({} respuestas):\".format(replies))\n",
    "    print(graph.node[most_replied_id])\n",
    "\n",
    "    # Buscamos la conversación más extensa\n",
    "    print(\"\\nConversación más extensa:\")\n",
    "    longest_path = nx.dag_longest_path(graph)\n",
    "    for tweet_id in longest_path:\n",
    "        node = graph.node[tweet_id]\n",
    "        print(\"{} (por {} el día {})\".format(node['tweet'], node['author'],node['created_at']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geolocalizar tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma atractiva de visualizar tweets es geolocalizarlos en un mapa, aunque no demasiados tweets tienen esta información.\n",
    "\n",
    "GeoJSON (http://geojson.org) es un formato común para estructuras de datos geográficos. Permite representar figuras geométricas, características y conjuntos de características. Las figuras geométricas solo contienen información de la forma (por ejemplo Punto, LineString, Polygon). Las características extienden este concepto incorporando propiedades adicionales.\n",
    "\n",
    "Una estructura GeoJSON es siempre un objeto JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos un conjunto de tweets y creamos una estructura GeoJSON\n",
    "tweets = \"stream_10000___Rusia2018_WorldCup_saved.jsonl\"\n",
    "with open(tweets, 'r') as f:\n",
    "    geo_data = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        try:\n",
    "            if tweet['coordinates']:\n",
    "                geo_json_feature = {\n",
    "                    \"type\": \"Feature\", \n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"Point\",\n",
    "                        \"coordinates\": tweet['coordinates']['coordinates']\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"text\": tweet['text'],\n",
    "                        \"created_at\": tweet['created_at']\n",
    "                    }\n",
    "                }\n",
    "                geo_data['features'].append(geo_json_feature)\n",
    "        except KeyError:\n",
    "            # Salteamos el tweet en caso de error\n",
    "            continue\n",
    "    # Guardamos el archivo GeoJSON\n",
    "    with open(\"tweets_map_test.geo.json\", 'w') as fout:\n",
    "        fout.write(json.dumps(geo_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folium\n",
    "\n",
    "Folium (https://folium.readthedocs.io/en/latest) es una biblioteca Python que permite generar mapas interactivo con muy poco esfuerzo.\n",
    "\n",
    "Para instalarlo en una distrubución anaconda, ejecutar en la consola `conda install -c conda-forge folium`. En caso de utilizar python de forma independiente ejecutar `pip install folium`\n",
    "\n",
    "El siguiente código muestra un mapa centrado en Tandil y con dos marcadores, uno para Tandil y otro para Mar del Plata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium \n",
    "\n",
    "sample_map = folium.Map(location=[-37.320480, -59.132904], zoom_start=7)\n",
    "# Marcador para Tandil\n",
    "tandil_marker = folium.Marker([-37.320480, -59.132904 ], popup='Tandil')\n",
    "tandil_marker.add_to(sample_map)\n",
    "# Marcador para Mar del Plata\n",
    "mardel_marker = folium.Marker([-37.979858, -57.589794], popup='Mar del plata')\n",
    "mardel_marker.add_to(sample_map)\n",
    "# Guardamos el mapa en un archivo HTML\n",
    "sample_map.save(\"mi_mapa.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geolocalizamos los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_map = folium.Map(location=[-37.320480, -59.132904], zoom_start=7)\n",
    "geojson_layer = folium.GeoJson(open(\"tweets_map.geo.json\",encoding = \"utf-8-sig\").read())\n",
    "geojson_layer.add_to(tweet_map)\n",
    "tweet_map.save(\"mapa_de_tweets.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos agrupar los tweets cercanos, podemos usar **MarkerCluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "tweet_map = folium.Map(location=[-37.320480, -59.132904], zoom_start=7)\n",
    "marker_cluster = MarkerCluster().add_to(tweet_map)\n",
    "geojson_layer = folium.GeoJson(open(\"tweets_map.geo.json\",encoding = \"utf-8-sig\").read())\n",
    "geojson_layer.add_to(marker_cluster)\n",
    "tweet_map.save(\"mapa_de_tweets_agrupados.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos también agregar información adicional a cada marcador para permitir, por ejemplo, ver el tweet exacto asociado a cada marcador.\n",
    "\n",
    "Un punto a aclarar para esto es que el objeto Marker interpreta las coordenadas como [Latitud, Longitud] mientras que GeoJSON utiliza el formato [Longitud, Latitud]. Es por esto que el arreglo de cooredenadas es invertido (*reverse()*) antes de definir el marcador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "tweet_map = folium.Map(location=[50, 5], zoom_start=5)\n",
    "marker_cluster = MarkerCluster().add_to(tweet_map)\n",
    "geodata = json.load(open(\"tweets_map.geo.json\"))\n",
    "for tweet in geodata['features']:\n",
    "    tweet['geometry']['coordinates'].reverse()\n",
    "    marker = folium.Marker(tweet['geometry']['coordinates'], popup=tweet['properties']['text'])\n",
    "    marker.add_to(marker_cluster)\n",
    "tweet_map.save(\"mapa_de_tweets_agrupados_con_tweets.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctico 3\n",
    "1. Definir el \"perfil de publicaciones\" de un usuario a partir de las publicaciones realizadas.\n",
    "2. Definir el perfil de intereses de un usuario a partir de la agregación de los \"perfiles de publicación\" de la gente que sigue\n",
    "3. Aplicar un algoritmo de clustering e intentar deducir los temas de interés de un usuario dado a partir de los resultados del mismo\n",
    "4. Probar para varios usuarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
