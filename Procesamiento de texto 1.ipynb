{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit learn (sklearn)\n",
    "\n",
    "Scikit-learn es una librería de código abierto para Python, que implementa una variedad de algoritmos de Minería de Datos, pre-procesamiento, referencias cruzadas y visualización usando una interfaz unificada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracción de características\n",
    "Utilizamos el módulo **sklearn.feature_extraction** para convertir documentos de texto (y de otros tipos) al espacio de vectores.\n",
    "Este módulo permite:\n",
    "    * Tokenizar strings y dar un identificador numérico a cada posible token, por ejemplo utilizando espacios y signos de puntuación como separadores\n",
    "    * Conteo de frecuencias de ocurrencia de tokens en cada documento\n",
    "    * Normalización y pesado para dar poco peso a los tokens que aparecen en la mayoría de los documentos\n",
    "\n",
    "Un corpus de documentos entonces es representado como una matriz, donde cada fila corresponde a un documento y cada columna a la presencia o no de un token. Este proceso se conoce como **vectorización** y la estrategia de tokenizar, contar y normalizar como **bag of words** (bolsa de palabras)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices dispersas\n",
    "Como la mayoría de los documentos suelen utilizar un subconjunto muy pequeño de las palabras utilizadas en el corpus, la matriz resultante tendrá muchos valores de características que son ceros.\n",
    "\n",
    "Por ejemplo, una colección de 10.000 documentos de texto corto (como correos electrónicos) usará un vocabulario con un tamaño del orden de 100.000 palabras únicas en total, mientras que cada documento usará de 100 a 1000 palabras únicas individualmente.\n",
    "\n",
    "Para poder almacenar dicha matriz en memoria pero también para acelerar las operaciones sobre ella, las implementaciones usan típicamente una representación dispersa tal como las implementaciones disponibles en el paquete **scipy.sparse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer\n",
    "La tokenización y el conteo de palabras en sklearn los realiza la clase CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo tiene muchos parámetros, pero los valores por defecto son generalmente aceptables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **analyzer** indica qué se considera un token: caracteres o palabras {‘word’, ‘char’, ‘char_wb’} . La opción 'char_wb' crea  n-gramas de caracteres solo desde el texto dentro de los límites de la palabra; Los n-grams en los bordes de las palabras se rellenan con espacio.\n",
    "Si se pasa una función en este parámetro, se utiliza para extraer la secuencia de características de la entrada en bruto sin procesar.\n",
    "\n",
    "El parámetro **binary** se usa si no queremos contar la cantidad de ocurrencias de cada palabra, sino solo su existencia (modelo binario)\n",
    "\n",
    "**lowercase** se indica si el texto debe convertirse a minúsculas antes de tokenizar\n",
    "\n",
    "**stopwords** recibe el string ‘english’, una lista de palabras, or None (default)\n",
    "\n",
    "**token_pattern** recibe una expresión regular que define lo que significa un token, y solo es usada si se utiliza el analizador de palabras ('word'). La expresión por defecto selecciona tokens con 2 o más caracteres alfanuméricos (los signos de puntuación se ignoran y se tratan siempre como separadores de palabras)\n",
    "\n",
    "**max_df**  es un número entre 0 y 1 (por defecto es 1) que indica que al construir el vocabulario se ignoren los términos que tienen una frecuencia de documento estrictamente mayor que el umbral dado (stopwords específicas del corpus). Si se da un número no entero, representa una proporción de documentos. Si se da un número entero, representa cantidad. Este parámetro se ignora si el vocabulario no es None \n",
    "\n",
    "De forma similar, **min_df** indica que se ignoren los términos que tienen una frecuencia strictamente menor al valor dado. Conocido como *cut-off* en la literatura\n",
    "\n",
    "**ngram_range** espera una tupla (min_n, max_n) con los valores mínimo y máximo de ngramas a extraer. Se usan todos los valores de n tal que *min_n <= n <= max_n*.\n",
    "\n",
    "**preprocessor** puede recibir una función que sobreescribe la etapa de preprocessamiento, pero mantiene la tokenización y la generación de ngramas \n",
    "\n",
    "**tokenizer** puede recibir una función que sobrescribe la tokenización de los strings, pero preserva el preprocesamiento y la generación de ngramas. Solo se aplica si *analyzer == 'word'*\n",
    "\n",
    "**max_features** Si es un número entero, distinto de None, se construye un vocabulario que solo considera las principales max_features, ordenadas por frecuencia. Se ignora si el vocabulario no es None\n",
    "\n",
    "**vocabulary** Espera opcionalmente un Mapping (por ejemplo un dict) donde las claves son términos y los valores son los índices en la matriz de características, o un iterable de terminos. Si no se proporciona este parámetro, el vocabulario se determina a partir de los documentos. Los índices en el mapping no deben repetirse y deben ser consecutivos (no debe haber ningún \"hueco\" entre 0 y el índice más largo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (1, 5)\t2\n",
      "  (1, 1)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 8)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 6)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función especifica que realiza la tokenización puede invocarse explícitamente. Notar que solo se consideran palabras de al menos dos letras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze('This is a text document to analyze')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada término encontrado por el analizador se le asigna un identificador entero único que corresponde a una columna en la matriz resultante. Esta interpretación de columnas puede obtenerse de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vectorizer.get_feature_names()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que el primer documento y el último poseen vectores idénticos, ya que utilizan las mismas palabras (aunque en diferente orden)\n",
    "\n",
    "El mapeo inverso puede realizarse inspeccionando la variable vocabulary_ del vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get('document')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras que no se observaron durante el entrenamiento, se ignoran en futuras llamadas al método transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 1,\n",
       " 'first': 0,\n",
       " 'is': 1,\n",
       " 'one': 0,\n",
       " 'second': 0,\n",
       " 'the': 0,\n",
       " 'third': 0,\n",
       " 'this': 2}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t =vectorizer.transform([\"This is a completely new document that was not observed previously in this dataset\"]).toarray()\n",
    "t\n",
    "dict(zip(features, t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"Something completely new\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para evitar perder la información del orden de las palabras (como ocurre con el primer documento y el último), podemos extraer bigramas, además de los 1-gramas (palabras individuales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                    token_pattern=r'\\b\\w+\\b', \n",
    "                                    min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: en Python la r delante de un string indica que las '\\' dentro del string se tomen como parte de la expresión regular, y no para escapar el caracter siguiente. Por lo tanto la expresión **r'\\b\\w+\\b'** es quivalente a **'\\\\\\\\b\\\\\\\\w+\\\\\\\\b'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' bi',\n",
       " 'big',\n",
       " 'igr',\n",
       " 'gra',\n",
       " 'ram',\n",
       " 'ams',\n",
       " 'ms ',\n",
       " ' ar',\n",
       " 'are',\n",
       " 're ',\n",
       " ' co',\n",
       " 'coo',\n",
       " 'ool',\n",
       " 'ol!',\n",
       " 'l! ']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_bigram_vect = CountVectorizer(ngram_range=(3, 3), analyzer='char_wb')\n",
    "analyze2 = char_bigram_vect.build_analyzer()\n",
    "analyze2('Bigrams are cool!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El vocabulario extraido con este vectorizador es mucho más grande y permite resolver ambigüedades codificadas en las posiciones locales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x21 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus)\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'and the',\n",
       " 'document',\n",
       " 'first',\n",
       " 'first document',\n",
       " 'is',\n",
       " 'is the',\n",
       " 'is this',\n",
       " 'one',\n",
       " 'second',\n",
       " 'second document',\n",
       " 'second second',\n",
       " 'the',\n",
       " 'the first',\n",
       " 'the second',\n",
       " 'the third',\n",
       " 'third',\n",
       " 'third one',\n",
       " 'this',\n",
       " 'this is',\n",
       " 'this the']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "bigram_vectorizer.get_feature_names()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular, la forma interrogativa \"Is this\" sólo está presente en el último documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "print(X_2[:,feature_index]) #obtengo toda la columna indicada por feature_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En algunos corpus pueden existir palabras que estén muy presentes y por lo tanto aporten poca información significativa acerca de los contenidos del documento.\n",
    "\n",
    "La transformación tf/idf se utiliza para pesar los términos inversamente a la cantidad de documentos en los que aparece la palabra\n",
    "\n",
    "$$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t, D)}$$\n",
    "$${\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}$$\n",
    "\n",
    " - **TfidfTransformer**: Normaliza una matriz de cuentas de palabras aplicando TF/IDF\n",
    " - **TfidfVectorizer**: Aplica sobre el corpus directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l1', smooth_idf=False, sublinear_tf=False,\n",
       "         use_idf=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False, norm='l1')\n",
    "transformer   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**norm**: 'l1', 'l2' o None, optional\n",
    "Se utiliza para normalizar los vectores de términos. Usar None para no normalizar.\n",
    "\n",
    "**use_idf**: boolean, default=True\n",
    "Habilita el uso de frecuencias inversas\n",
    "\n",
    "**smooth_idf**: boolean, default=True\n",
    "Suaviza los pesos idf agregando 1 a las frecuencias de los documentos, como si existiera un documento extra que contiene el término exactamente 1 vez. Evita divisiones por cero. \n",
    "\n",
    "**sublinear_tf**: boolean, default=False\n",
    "Aplica escalado tf sublineal, es decir, reemplaza tf con 1 + log(tf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el siguiente ejemplo en el que el primer término se encuentra presente en todos los documentos por lo que no es representativo. Los otros dos términos se encuentran en menos del 50% de los documentos y por lo tanto podrían ser representativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [[3, 0, 1],\n",
    "          [2, 0, 0],\n",
    "          [3, 0, 0],\n",
    "          [4, 0, 0],\n",
    "          [3, 2, 0],\n",
    "          [3, 0, 2]]\n",
    "\n",
    "tfidf = transformer.fit_transform(counts)\n",
    "tfidf                         \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5883954 , 0.        , 0.4116046 ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.34950701, 0.65049299, 0.        ],\n",
       "       [0.41682734, 0.        , 0.58317266]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada fila está normalizada para que sume 1 aplicando la norma euclídea (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer2 = TfidfTransformer()\n",
    "transformer2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, smooth_idf es True, por lo que se agrega 1 al numerador y al denominador como si existira un documento extra que tiene el término exactamente 1 vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85151335, 0.        , 0.52433293],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [1.        , 0.        , 0.        ],\n",
       "       [0.55422893, 0.83236428, 0.        ],\n",
       "       [0.63035731, 0.        , 0.77630514]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer2.fit_transform(counts).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pesos de cada término calculado por el método fit se almacenan en un atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 2.79175947, 2.09861229])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 2.25276297, 1.84729786])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer2.idf_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tf/idf se utiliza frecuentemente para texto, existe otra clase llamada **TfidfVectorizer** que combina todas las opciones de CountVectorizer y TfidfTranformer en un único modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer3 = TfidfVectorizer()\n",
    "X_3 = vectorizer3.fit_transform(corpus)\n",
    "X_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674],\n",
       "       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
       "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
       "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
       "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
       "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selección de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un ejemplo de selección de características es la Información Mutua (MI). Esta métrica calculada para dos variables aleatorias es un número no negativo que mide la dependencia entre ambas variables. Es igual a 0 si y solo si las dos variables son independientes y los valores mayores significan dependencia.\n",
    "\n",
    "$${\\displaystyle I(X;Y)=\\sum _{y\\in Y}\\sum _{x\\in X}p(x,y)\\log {\\left({\\frac {p(x,y)}{p(x)\\,p(y)}}\\right)}}$$ (X son las características e Y las clases)\n",
    "$${\\displaystyle I(Atributo;Clase)=H(Clase) - H(Clase | Atributo)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5.551115123125783e-17),\n",
       " ('and', 0.5623351446188083),\n",
       " ('document', 0.5623351446188083),\n",
       " ('is', 0.5623351446188083),\n",
       " ('one', 0.5623351446188083),\n",
       " ('second', 0.5623351446188083),\n",
       " ('third', 0.5623351446188083),\n",
       " ('this', 0.5623351446188083),\n",
       " ('first', 0.6931471805599452)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "clases = ['A','B','C','A']\n",
    "cv = CountVectorizer()\n",
    "X_vec = cv.fit_transform(corpus)\n",
    "res = dict(zip(cv.get_feature_names(),\n",
    "               mutual_info_classif(X_vec, clases, discrete_features=True)\n",
    "               ))\n",
    "\n",
    "#Imprimimos ordenados por MI\n",
    "from operator import itemgetter\n",
    "sorted(res.items(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
